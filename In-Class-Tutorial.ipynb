{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Tutorial for CHEGs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the in-class portion of the ML tutorial! \n",
    "\n",
    "Don't worry, you don't need any knowledge of Python to be able to do this, we'll walk you through every step. \n",
    "\n",
    "Just follow the comments!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The first step in creating a robot that can do your homework is to teach it to recognize the numbers on your problem set.**\n",
    "\n",
    "How would we go about teaching a computer to recognize digits?\n",
    "\n",
    "Well, it probably would be pretty hard to write an algorithm to do it directly. Maybe your teacher has messy handwriting, or they write really lightly, or a number of other factors. Instead, we will try to teach the computer the same way you might teach a kindergartener - by showing it many examples of each digit and telling it what digit that is.\n",
    "\n",
    "This *data driven* approach requires we have a *training set* of many handwritten numbers to teach the computer. Luckily, we have one below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm a comment, hi there!\n",
    "# Read through the code until the end and follow any instructions in the comments!\n",
    "# It's ok if you don't understand what everything does.\n",
    "\n",
    "# Imports the datasets, classifier, and metrics\n",
    "from sklearn import datasets, neighbors, metrics\n",
    "\n",
    "# Plotting and math packages for Python\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "# Saves the datasets containing the labeled digits\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# Click on this box and press Ctrl+Enter on your keyboard to run this code!\n",
    "# (the \"Ctrl\" and \"Enter\" keys, not the \"+\" key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each digit in the dataset is stored as an array of pixel values, as demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to see how much data we have\n",
    "print('(Datapoints, Attributes)')\n",
    "print(digits.data.shape, '\\n')\n",
    "\n",
    "# An example of the data in the set\n",
    "print('For example, this is the array form of the first digit in the set.')\n",
    "print('Note it is composed of 8 separate 1x8 arrays.')\n",
    "print('We can think of it as a 8x8 pixel image.')\n",
    "print(digits.images[0], '\\n')\n",
    "\n",
    "# Reshapes the data so we can use it for training, validation, and testing later\n",
    "# Data goes from an array of arrays to just one array\n",
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "\n",
    "# The example reshaped into a form that can be used by our algorithm\n",
    "print('The first digit in the set has been reformatted so it can be used by our ML algorithm.')\n",
    "print('Note that the one datapoint now is a single 1x64 array.')\n",
    "print(data[0, :], '\\n')\n",
    "\n",
    "# Setting up our images and labels to be shown\n",
    "print('And these are the first few digits in the dataset with their corresponding labels')\n",
    "images_and_labels = list(zip(digits.images, digits.target))\n",
    "\n",
    "# Visualizing the first few images in the dataset\n",
    "for index, (image, label) in enumerate(images_and_labels[:10]):\n",
    "# Runs through the first 10 images and labels    \n",
    "    \n",
    "    plt.subplot(2, 5, index + 1) # Initializes plot\n",
    "    plt.axis('off') # No axes\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    # Shows each image in grayscale on white background\n",
    "    plt.title('Label: %i' % label) # Sets labels\n",
    "    \n",
    "# Press Ctrl+Enter again!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from the above exploration of the data, we see we have 1797 handwritten digits (rows of data samples), each represented as an 8x8 array of pixels.\n",
    "\n",
    "In the dataset we will use, we have reformatted each 8x8 array of pixels as a 1x64 array of 64 different variables, or *attributes*. Here, each row represents a handwritten digit and each column represents one of the pixels of that handwritten digit.\n",
    "\n",
    "**Machine learning techniques such as this one can be applied to many different cases - you just need to change what attributes are in the columns of your data!**\n",
    "\n",
    "For instance, if you wanted to predict what clear liquid you had in a bottle, you might have attributes for density, vapor pressure, whether it dissolves in water, or others.\n",
    "\n",
    "Or, if you wanted to predict the probability of Dr. Burkey's chemical plant blowing up, you might have attributes for how peppy Emily is, how annoying Victor is being, whether Charles has to take care of his wife, and how much pressure Wanda is putting on you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The digits dataset above, our training set, will be used to train our model, specifically the k-Nearest Neighbor classifier.** This classifier will be able to tell us what digit any new (unlabeled) input (that it has never seen before) is, as long as it is in the same 8x8 format as the training samples. We hope that many of the predictions of the classifier line up with what the digits actually are, the *ground truth*.\n",
    "\n",
    "Before training our model, we will need to separate our dataset into a *training set* (which the classifier will learn from), a *validation set* (which we can use to test our initial results and fine-tune the value of k), and a *testing set* (which we will hide from the classifier until we are confident we have a good model). Splitting up our dataset like this helps us avoid *overfitting*, where the model has great performance on this dataset, but does much worse when exposed to new inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALERT: you have to change some of the numbers here to get the code to work\n",
    "\n",
    "# Remember there are 1797 datapoints to distribute among all these categories!\n",
    "# We will set the training set as datapoints 0 through trainEnd (you specify trainEnd below)\n",
    "# The validation set will be datapoints trainEnd through valEnd (you specify valEnd below)\n",
    "# Datapoints valEnd till the end will be the testing set\n",
    "\n",
    "# Aim for around 40-80% training data, ~10% validation data, and 10-50% testing data.\n",
    "\n",
    "# Training datapoints\n",
    "trainEnd = #Put number here\n",
    "trainingDigits = data[:trainEnd] # Sets the images\n",
    "trainingLabels = digits.target[:trainEnd] # Sets the correspoinding labels\n",
    "\n",
    "\n",
    "# Validation datapoints\n",
    "valEnd = #Put number here\n",
    "validationDigits = data[trainEnd:valEnd]\n",
    "validationLabels = digits.target[trainEnd:valEnd]\n",
    "\n",
    "# Testing datapoints\n",
    "testingDigits = data[valEnd:]\n",
    "testingLabels = digits.target[valEnd:]\n",
    "\n",
    "# Press Ctrl+Enter when you've adjusted the sets to your liking!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the k-Nearest Neighbors classifier. This classifier has a number of settings that can be optimized to make it recognize digits better. These settings are called *hyperparamters* As we discussed, the *hyperparameter* you have to optimize is k.\n",
    "\n",
    "k represents the number of neighbors that we consider when deciding what digit to assign a new input. **We will try tweaking the value of k to figure out what works best on the validation set.** Then, once you are satisfied with a value of k, we can run the classifier on the testing set and report our accuracy.\n",
    "\n",
    "Be careful! Do not try to optimize, or else you are effectively training your model on that set as well. Then the results of our training might not be *generalizable*, say if we wrote our own digits and had the classifier try to categorize those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALERT: you have to change some of the numbers here to get the code to work\n",
    "\n",
    "# We want to test a bunch of values \n",
    "# For instance, [1, 3, 5, 100]\n",
    "for k in [#Put numbers here]:\n",
    "    \n",
    "    # Sets up our model with the appropriate k\n",
    "    neigh = neighbors.KNeighborsClassifier(n_neighbors = k)\n",
    "  \n",
    "    # Fits the classifier and has it predict labels\n",
    "    neigh.fit(trainingDigits, trainingLabels)\n",
    "    validationPredictions = neigh.predict(validationDigits)\n",
    "    \n",
    "    # Calculating and printing some of the metrics for our data\n",
    "    acc = np.mean(validationPredictions == validationLabels) # Calculates accuracy\n",
    "    print('k Value: %i' % k)\n",
    "    print('Accuracy: %f' % acc, '\\n')\n",
    "    print(metrics.classification_report(validationLabels, validationPredictions))\n",
    "    # A summary of some testing metrics\n",
    "\n",
    "# Press Ctrl+Enter when you're ready!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've picked your favorite k-value, use it for the testing below! **Make sure you run the testing set only once, or you risk overfitting your hyperparameter k!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALERT: you have to change some of the numbers here to get the code to work\n",
    "\n",
    "# Pick the k-value you think did best above\n",
    "k = # Put number here\n",
    "\n",
    "# Sets up the classifier\n",
    "neigh = neighbors.KNeighborsClassifier(n_neighbors = k)\n",
    "\n",
    "# Fits the classifier and has it predict labels\n",
    "neigh.fit(trainingDigits, trainingLabels)\n",
    "testingPredictions = neigh.predict(testingDigits)\n",
    "\n",
    "# Calculating and printing some of the metrics for our data\n",
    "acc = np.mean(testingPredictions == testingLabels) # Calculates accuracy\n",
    "print('k Value: %i' % k) \n",
    "print('Accuracy: %f' % acc, '\\n')\n",
    "print(metrics.classification_report(testingLabels, testingPredictions))\n",
    "# A summary of some testing metrics\n",
    "\n",
    "# Press Ctrl+Enter when you're ready!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job! Hopefully that was pretty good accuracy! Some of the numbers that the classifier predicted are visualized below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up images and labels of our testing set\n",
    "print('Here are some examples of what our classifier predicted!')\n",
    "images_and_labels = list(zip(digits.images[valEnd:], digits.target[valEnd:]))\n",
    "\n",
    "# Visualizing the last few images in the testing set\n",
    "# Runs through the first 10 images and labels\n",
    "for index, (image, label) in enumerate(images_and_labels[:10]):\n",
    "    plt.subplot(2, 5, index + 1) # Initializes plot\n",
    "    plt.axis('off') # No axes\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    # Shows each image in grayscale on white background\n",
    "    plt.title('Predict: %i' % label) # Sets labels\n",
    "\n",
    "# Press Ctrl+Enter when you're ready!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While k-Nearest Neighbors was a pretty good choice for this task, it does have some pros and cons.\n",
    "\n",
    "**Pros:**\n",
    "- Easy to understand\n",
    "- Little training time, since all you have to do is store all the datapoints\n",
    "\n",
    "**Cons:**\n",
    "- Takes more time to classify since you need to compare every new testing sample to all of the training samples\n",
    "\n",
    "\n",
    "\n",
    "Also, many similar classification ML algorithms all suffer from some of the same pitfalls.\n",
    "- Turning an image upside down, shifting it, distorting it, or darkening it could make it fail miserably (after all, it's just learning from pixel values).\n",
    "\n",
    "This could make it dangerous when you get different quality data!\n",
    "\n",
    "\n",
    "\n",
    "**Still, you've now got your homework-doing robot able to read numbers! Now all you have to do is teach it to do a Laplace transform!**\n",
    "\n",
    "Thanks for trying our tutorial!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Adapted from cs231n and Gael Varoquaux's example on the scikit-learn website. (Thanks to them for the inspiration and some code).*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
